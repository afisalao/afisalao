# -*- coding: utf-8 -*-
"""Decission_Trees&Log_reg_on_INNHotels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YoNFw4zr7VS7ISQ6hxzunVu_BfI8ewIl

# INN Hotels Project


## Context

A significant number of hotel bookings are called off due to cancellations or no-shows. The typical reasons for cancellations include change of plans, scheduling conflicts, etc. This is often made easier by the option to do so free of charge or preferably at a low cost which is beneficial to hotel guests but it is a less desirable and possibly revenue-diminishing factor for hotels to deal with. Such losses are particularly high on last-minute cancellations.

The new technologies involving online booking channels have dramatically changed customers’ booking possibilities and behavior. This adds a further dimension to the challenge of how hotels handle cancellations, which are no longer limited to traditional booking and guest characteristics.

The cancellation of bookings impact a hotel on various fronts:
1. Loss of resources (revenue) when the hotel cannot resell the room.
2. Additional costs of distribution channels by increasing commissions or paying for publicity to help sell these rooms.
3. Lowering prices last minute, so the hotel can resell a room, resulting in reducing the profit margin.
4. Human resources to make arrangements for the guests.

**Objective**

The increasing number of cancellations calls for a Machine Learning based solution that can help in predicting which booking is likely to be canceled. INN Hotels Group has a chain of hotels in Portugal, they are facing problems with the high number of booking cancellations and have reached out to your firm for data-driven solutions. You as a data scientist have to analyze the data provided to find which factors have a high influence on booking cancellations, build a predictive model that can predict which booking is going to be canceled in advance, and help in formulating profitable policies for cancellations and refunds.


**Data Description**

The data contains the different attributes of customers' booking details. The detailed data dictionary is given below.


**Data Dictionary**

1. Booking_ID: the unique identifier of each booking
2. no_of_adults: Number of adults
3. no_of_children: Number of Children
4. no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel
5. no_of_week_nights: Number of weeknights (Monday to Friday) the guest stayed or booked to stay at the hotel
6. type_of_meal_plan: Type of meal plan booked by the customer:

    a. Not Selected – No meal plan selected

    b. Meal Plan 1 – Breakfast

    c. Meal Plan 2 – Half board (breakfast and one other meal)

    d. Meal Plan 3 – Full board (breakfast, lunch, and dinner)

7. required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)
8. room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels Group
9. lead_time: Number of days between the date of booking and the arrival date
10. arrival_year: Year of arrival date
11. arrival_month: Month of arrival date
12. arrival_date: Date of the month
13. market_segment_type: Market segment designation.
14. repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)
15. no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking
16. no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking
17. avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)
18. no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)
19. booking_status: Flag indicating if the booking was canceled or not.

# Import Dataset

Libraries
"""

import warnings
warnings.filterwarnings('ignore')

from statsmodels.tools.sm_exceptions import ConvergenceWarning

warnings.simplefilter('ignore', ConvergenceWarning)

# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np

# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option('display.max_columns', None)
# Sets the limit for the number of displayed rows
pd.set_option('display.max_rows', 200)
# setting the precision of floating numbers to 5 decimal points
pd.set_option('display.float_format', lambda x: '%.5f' % x)

# Library to split data
from sklearn.model_selection import train_test_split

# To build model for prediction
import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from statsmodels.tools.tools import add_constant
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# To tune different models
from sklearn.model_selection import GridSearchCV


# To get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,
    plot_confusion_matrix,
    precision_recall_curve,
    roc_curve,
    make_scorer,
)

"""Loading the Dataset"""

# mount Google Drive to upload data from the drive
from google.colab import drive
drive.mount('/content/drive')

# download, read & assign data to a variable
data = pd.read_csv('/content/drive/MyDrive/DataSets/INNHotelsGroup.csv')

# copying data to another variable to avoid any changes to original data
logdata = data.copy()
trdata = data.copy()

"""Examining the first and last 5 rows of the dataset"""

data.head()

data.tail()

"""Identifying the shape of the dataset"""

data.shape

"""The data has 3,675 observationa and 19 features.

Examining the different types of data in the dataset's columns
"""

data.info()

"""The data has 19 features, 5 of these features (Booking ID, Type of meal plan, Room Type reserved, the market segment and Booking Status all have categorical variables. The remaining features (14) have either discreet or continous variables as observations within their features."""

# check for missing values
data.isnull().sum()

"""The data has nin missing values."""

# checking for duplicate values
data.duplicated().sum()

"""The data has no duplicate observations

Removing the Booking ID column out of the dataset
"""

data = data.drop(['Booking_ID'], axis = 1)

data.head()

# Assessing the data's statistical summary
data.describe().T

"""The distribution of the dataset follows a normal curve. Some features, such as the number of adults or the arrival date, have left skewed distributions. Other features, such as lead time and average room cost, seem to exhibit right-skewed distributions.

# Exploratory Data Analysis **(for more comprehensive EDA [See Appendix](https://colab.research.google.com/drive/1-F1nw15V9MmkIyEJtYm2tf7k1CemIASP#scrollTo=m64iAshiYHf3))**

### **Univariate Analysis**
"""

sns.set_style('darkgrid')
data.hist(figsize= (15,10))
plt.show()

"""Distribution of observed variables in the datasets.

Q1. **A line plot showing the busiest month for INN Hotels**
"""

# grouping the data on arrival months with the count of bookings
monthly_data = data.groupby(['arrival_month'])['booking_status'].count()

# creating a dataframe with months and count of customers in each month
monthly_data = pd.DataFrame(
    {'Month': list(monthly_data.index), 'Guests': list(monthly_data.values)}
)

# plotting the trend over different months
plt.figure(figsize=(10, 5))
sns.lineplot(data=monthly_data, x='Month', y='Guests')
plt.show()

"""October is the month with the highest number of check-in, followed by september and august respectively.

Q2. No of Guests by Market Segments
"""

# plot a visual representation
data['total_guests'] = data['no_of_adults'] + data['no_of_children']

pd.crosstab(data['market_segment_type'],
            data['total_guests'],
            normalize='index').plot(kind='bar',
            figsize=(17,12),
            stacked = False
            )
plt.legend()
plt.title('Number of Guests by Market Segments')
plt.xlabel('Type of Market Segments')
plt.ylabel('Number of Guests')
plt.show()

"""Online and offline reservations have guests mostly from two to four guests, there are some instance where there are single guest.

Most single guest reservations are made by the Aviation sector.

"""

data.drop('total_guests', axis = 1, inplace = True)

"""Q3. **Boxplot showing the prices of rooms by market segment**"""

plt.figure(figsize=(10, 6))
sns.boxplot(
    data=data, x = 'market_segment_type', y = 'avg_price_per_room',
)
plt.show()

"""1. Reservations made trough comlimentary services have the lowest average price per room.

2. Corporate, Aviation, Online and Offline reservations have comparable average price per room

Q4. **Percentage of Cancellations**
"""

# create a countplot with percentage
plt.figure(figsize=(10, 5))
ax = sns.countplot(data=data, x = 'booking_status')

total = len(data['booking_status'])
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2 - 0.1
    y = p.get_y() + p.get_height()
    ax.annotate(percentage, (x, y))

# add labels and title
plt.title('Percentage of Cancellations')
plt.xlabel('Cancellations')
plt.ylabel('Percentage')
plt.show()

percentages = data['booking_status'].value_counts(normalize=True) * 100

# create a bar chart
plt.bar(percentages.index,
        percentages.values)
plt.xlabel('Values')
plt.ylabel('Percentage')
plt.title('Percentage of Cancellations')

# display the chart
plt.show()

"""\Reservations are canceled at an estimated rate of 30%.

Q5. **percentage of Cancellations by repeated Guests**
"""

count = data['repeated_guest'].nunique()
sorter = data['booking_status'].value_counts().index[-1]
tab1 = pd.crosstab(data['repeated_guest'], data['booking_status'], margins=True).sort_values(
        by=sorter, ascending=False
    )
print(tab1)
print('-' * 120)
tab = pd.crosstab(data['repeated_guest'], data['booking_status'], normalize = 'index').sort_values(
        by=sorter, ascending=False
    )
tab.plot(kind='bar', stacked = False, figsize=(count + 5, 5))
plt.legend(
        loc = 'lower left', frameon = False,
    )
plt.legend(loc = 'upper left', bbox_to_anchor=(1, 1))
plt.show()

"""Q6. **Impact of Special Requirements on Cancellations**"""

plt.figure(figsize=(10, 6))
sns.boxplot(
    data=data, y = 'no_of_special_requests',x = 'booking_status',
)
plt.show()

"""The boxplot shows no signid=ficant difference of cancellation with refereence to special requests made by customers.

### **Bivariate Analysis**
"""

cols_list = data.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(12, 7))
sns.heatmap(
    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt='.2f', cmap='Spectral'
)
plt.show()

"""The features don't seem to have any strong linear traits among themselves.

# Data Prepossing

Detecting Outliers
"""

# outlier detection using boxplot
numeric_columns = data.select_dtypes(include=np.number).columns.tolist()


plt.figure(figsize=(17, 12))

for i, variable in enumerate(numeric_columns):
    plt.subplot(5, 5, i + 1)
    sns.boxplot(data = data, x=variable)
    plt.tight_layout(pad = 2)
    plt.title(variable)

plt.show()

"""There are outliers for some features. Outliers will be considered in this study because there is no reason to think that they represent inaccurate data.

For the purpose of data analysis, the booking status columns are encoded as 1 for cancelled and 0 for Not cancelled.
"""

# Encode Canceled bookings to 1 and Not_Canceled as 0 for further analysis
data['booking_status'] = data['booking_status'].apply(lambda x: 1 if x == 'Canceled' else 0)

"""# Model Building

We will apply a user-defined function to establish the metrics for the analysis.

The model's accuracy can be a good indicator of how well it can forecast the possibility that bookings will be canceled. But, the model's misclassification is what interests us. It is preferable to include both Precision and Recall in this situation since the cost of false positives and false negatives would still have a major influence on INN service. The F1 score, which would take into consideration both false positives and false negatives, would therefore be the proper assessment.

However, we will continue to use all of the evaluation criteria in this analysis.
"""

# defining a function to compute different metrics to check performance of a classification model built using statsmodels
def model_performance_classification_statsmodels(
    model, predictors, target, threshold=0.5
):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    threshold: threshold for classifying the observation as class 1
    """

    # checking which probabilities are greater than threshold
    pred_temp = model.predict(predictors) > threshold
    # rounding off the above values to get classes
    pred = np.round(pred_temp)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},
        index=[0],
    )

    return df_perf

# defining a function to plot the confusion_matrix of a classification model


def confusion_matrix_statsmodels(model, predictors, target, threshold=0.5):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    threshold: threshold for classifying the observation as class 1
    """
    y_pred = model.predict(predictors) > threshold
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

"""## Logistic Regression (with statsmodels library)

#### Data Preparation for modeling (Logistic Regression)

The dataset will be divided  into train and test groups, to evaluate the model we develop using the train data.

The categorized features will be encoded.

The model's goal is to predict which reservations will be canceled.
"""

X = data.drop(['booking_status'], axis=1)
y = data['booking_status']

# adding constant
X = sm.add_constant(X)

X = pd.get_dummies(X,drop_first=True,)

# Splitting data in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)

print('Shape of Training set : ', X_train.shape)
print('Shape of test set : ', X_test.shape)
print('Percentage of classes in training set:')
print(y_train.value_counts(normalize=True))
print('Percentage of classes in test set:')
print(y_test.value_counts(normalize=True))

"""#### Building Logistic Regression Model

Building a Logistic Regression Model
"""

# fitting logistic regression model
logit = sm.Logit(y_train, X_train.astype(float))
lg = logit.fit()

print(lg.summary())

"""There are 25,392 observations in the train set.

Some variable have high p-values.
"""

print('Training performance:')
model_performance_classification_statsmodels(lg, X_train, y_train)

"""We have about 80% accuracy on the train data. With a F1 score of 0.67, which takes into account the false positives and false negatives, with a threeshold of 0.5.

#### Multicollinearity
"""

# we will define a function to check VIF
def checking_vif(predictors):
    vif = pd.DataFrame()
    vif['feature'] = predictors.columns

    # calculating VIF for each feature
    vif['VIF'] = [
        variance_inflation_factor(predictors.values, i)
        for i in range(len(predictors.columns))
    ]
    return vif

checking_vif(X_train)

"""The model doesn't have any multicollinearity. The variables' coefficients are acceptable.

#### **Dropping independent variables with high p-value as they are not statistically significant to the target**

Ho: Population Beta is equal to Zero (βi=0)

Ha: Population Beta is not equal to Zero (βi≠0)

We will drop the predictors with high p-values as they do not have any statistical significance to the regression. In this case, we will fail to reject the null hypothesis that beta is equal to zero and consequently these predictors have no relationship with the target.
"""

# initial list of columns
cols = X_train.columns.tolist()

# setting an initial max p-value
max_p_value = 1

while len(cols) > 0:
    # defining the train set
    x_train_aux = X_train[cols]

    # fitting the model
    model = sm.Logit(y_train, x_train_aux).fit(disp = False)

    # getting the p-values and the maximum p-value
    p_values = model.pvalues
    max_p_value = max(p_values)

    # name of the variable with maximum p-value
    feature_with_p_max = p_values.idxmax()

    if max_p_value > 0.05:
        cols.remove(feature_with_p_max)
    else:
        break

selected_features = cols
print(selected_features)

X_train1 = X_train[selected_features]
X_test1 = X_test[selected_features]

logit1 = sm.Logit(y_train,X_train1.astype(float))
lg1 = logit1.fit()
print(lg1.summary())

"""There are no longer any variables in the model that are not significantly important to the analysis.


"""

print("Training performance:")
model_performance_classification_statsmodels(lg1, X_train1, y_train)

"""We see no significant change in our metrics after eliminating values with high P-values, which was anticipated given that the variables with high p-values are not statistically significant for the analysis.

####  **Converting coefficients to odds**
"""

# converting coefficients to odds
odds = np.exp(lg1.params)

# finding the percentage change
perc_change_odds = (np.exp(lg1.params) - 1) * 100

# removing limit from number of columns to display
pd.set_option('display.max_columns', None)

# adding the odds to a dataframe
pd.DataFrame({'Odds': odds, 'Change_odd%': perc_change_odds}, index=X_train1.columns).T

"""1.	Number of previous cancellations: The odds ratio for this variable is 1.2, indicating that for each additional previous cancellation, the odds of canceling the current reservation increase by 20.9%. This suggests that guests who have a history of canceling their reservations are more likely to cancel their current reservation as well.
2.	Number of special requests: The odds ratio for this variable is 1.1, indicating that for each additional special request made by the guest, the odds of canceling the reservation increase by 15.7%. This suggests that guests who have more specific needs or requirements are more likely to cancel their reservation if those demands are not honored.
3.	Number of weekend nights: The odds ratio for this variable is 1.2, indicating that for each additional weekend night booked, the odds of canceling the reservation increase by 15.8%. This suggests that guests who book for weekend nights are more likely to cancel their reservation compared to guests who book for weekdays.
4.	Type of meal plan selected: Guests who selected Meal Plan 2 have higher odds of canceling their reservation compared to guests who did not select any meal plan. The odds ratio for Meal Plan 2 is 1.2, indicating that the odds of canceling the reservation for guests who selected this meal plan are 23.7% higher compared to guests who did not select any meal plan.
5.	Lead time: The odds ratio indicates that for each additional lead time day, the odds of a cancellation decrease by approximately 1.5%. This suggests that guests who book their reservation further in advance are less likely to cancel their reservation compared to guests who book closer to their arrival date.
6.	Number of adults: The odds ratio for this variable is 1.1, indicating that for each additional adult in the reservation, the odds of canceling the reservation increase by 10.8%. This suggests that guests who have more adults in their party are more likely to cancel their reservation.
7.	Number of children: The odds ratio for this variable is 1.1, indicating that for each additional child in the reservation, the odds of canceling the reservation increase by 16.4%. This suggests that guests who have more children in their party are more likely to cancel their reservation.

#### Checking model performance on the training set
"""

# creating confusion matrix
confusion_matrix_statsmodels(lg1, X_train1, y_train)

"""From the confusion matrix, about 60% will not cancel their reservations, 20% will cancel their reservation. This are the true positives and True negatives, which account for 80% of the prediction and also indicated in the accuracy of the model. The other 20% were the misclassified predictions in the model,"""

print('Train performance:')
log_reg_model_train_perf = model_performance_classification_statsmodels(lg1, X_train1, y_train)
log_reg_model_train_perf

"""#### ROC-AUC
* ROC-AUC on training set
"""

logit_roc_auc_train = roc_auc_score(y_train, lg1.predict(X_train1))
fpr, tpr, thresholds = roc_curve(y_train, lg1.predict(X_train1))
plt.figure(figsize=(10, 7))
plt.plot(fpr, tpr, label= 'Logistic Regression (area = %0.2f)' % logit_roc_auc_train)
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.01])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc = 'lower right')
plt.show()

"""#### Model Performance Improvement

#### Optimal threshold using AUC-ROC curve
"""

# Optimal threshold as per AUC-ROC curve
# The optimal cut off would be where tpr is high and fpr is low
fpr, tpr, thresholds = roc_curve(y_train, lg1.predict(X_train1))

optimal_idx = np.argmax(tpr - fpr)
optimal_threshold_auc_roc = thresholds[optimal_idx]
print(optimal_threshold_auc_roc)

# creating confusion matrix
confusion_matrix_statsmodels(
    lg1, X_train1, y_train, threshold= optimal_threshold_auc_roc
)

# checking model performance for this model
log_reg_model_train_perf_threshold_auc_roc = model_performance_classification_statsmodels(
    lg1, X_train1, y_train, threshold=optimal_threshold_auc_roc
)
print('Training performance:')
log_reg_model_train_perf_threshold_auc_roc

"""#### Let's use Precision-Recall curve and see if we can find a better threshold"""

y_scores = lg1.predict(X_train1)
prec, rec, tre = precision_recall_curve(y_train, y_scores,)


def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], 'b--', label = 'precision')
    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')
    plt.xlabel('Threshold')
    plt.legend(loc = 'upper left')
    plt.ylim([0, 1])


plt.figure(figsize=(10, 7))
plot_prec_recall_vs_tresh(prec, rec, tre)
plt.show()

# setting the threshold
optimal_threshold_curve = 0.42

"""#### Checking model performance on training set"""

# creating confusion matrix
confusion_matrix_statsmodels(
    lg1, X_train1, y_train, threshold = optimal_threshold_curve
)

log_reg_model_train_perf_threshold_curve = model_performance_classification_statsmodels(
    lg1, X_train1, y_train, threshold=optimal_threshold_curve
)
print('Training performance:')
log_reg_model_train_perf_threshold_curve

"""#### Let's check the performance on the test set

**Using model with default threshold**
"""

# creating confusion matrix
confusion_matrix_statsmodels(lg1, X_test1, y_test, threshold = optimal_threshold_curve)

log_reg_model_test_perf = model_performance_classification_statsmodels(
    lg1, X_test1, y_test, threshold = optimal_threshold_curve)

print('Test performance:')
log_reg_model_test_perf

"""* ROC curve on test set"""

logit_roc_auc_train = roc_auc_score(y_test, lg1.predict(X_test1))
fpr, tpr, thresholds = roc_curve(y_test, lg1.predict(X_test1))
plt.figure(figsize=(10, 7))
plt.plot(fpr, tpr, label = 'Logistic Regression (area = %0.2f)' % logit_roc_auc_train)
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.01])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc = 'lower right')
plt.show()

"""**Using model with threshold=0.37**"""

# creating confusion matrix
confusion_matrix_statsmodels(lg1, X_test1, y_test, threshold=optimal_threshold_auc_roc)

# checking model performance for this model
log_reg_model_test_perf_threshold_auc_roc = model_performance_classification_statsmodels(
    lg1, X_test1, y_test, threshold=optimal_threshold_auc_roc
)
print('Test performance:')
log_reg_model_test_perf_threshold_auc_roc

"""**Using model with threshold = 0.42**"""

# creating confusion matrix
confusion_matrix_statsmodels(lg1, X_test1, y_test, threshold=optimal_threshold_curve)

log_reg_model_test_perf_threshold_curve = model_performance_classification_statsmodels(
    lg1, X_test1, y_test, threshold=optimal_threshold_curve
)
print('Test performance:')
log_reg_model_test_perf_threshold_curve

"""#### Model performance summary"""

# training performance comparison

models_train_comp_df = pd.concat(
    [
        log_reg_model_train_perf.T,
        log_reg_model_train_perf_threshold_auc_roc.T,
        log_reg_model_train_perf_threshold_curve.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    'Logistic Regression-default Threshold',
    'Logistic Regression-0.37 Threshold',
    'Logistic Regression-0.42 Threshold',
]

print('Training performance comparison:')
models_train_comp_df

# test performance comparison

models_test_comp_df = pd.concat(
    [
        log_reg_model_test_perf.T,
        log_reg_model_test_perf_threshold_auc_roc.T,
        log_reg_model_test_perf_threshold_curve.T,
    ],
    axis=1,
)
models_test_comp_df.columns = [
    'Logistic Regression-default Threshold',
    'Logistic Regression-0.37 Threshold',
    'Logistic Regression-0.42 Threshold',
]

print('Test performance comparison:')
models_test_comp_df

"""On training and test data, nearly all three models perform well without an overfitting problem.

The threeshold for the model with the greatest F1 score is 0.37. It should therefore be selected as the final model.

## Decision Tree

#### Data Preparation for modeling (Decision Tree)
"""

X = data.drop(['booking_status'], axis=1)
y = data['booking_status']

X = pd.get_dummies(X, drop_first=True)

# Splitting data in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)

print('Shape of Training set : ', X_train.shape)
print('Shape of test set : ', X_test.shape)
print('Percentage of classes in training set:')
print(y_train.value_counts(normalize=True))
print('Percentage of classes in test set:')
print(y_test.value_counts(normalize=True))

"""**Utilize a user-defined function to generate a confusion matrix and various metrics. **

* The model_performance_classification_sklearn function will be used to check the model performance of models.
* The confusion_matrix_sklearnfunction will be used to plot the confusion matrix.
"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {'Accuracy': acc, 'Recall': recall, 'Precision': precision, 'F1': f1,},
        index=[0],
    )

    return df_perf

def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ['{0:0.0f}'.format(item) + '\n{0:.2%}'.format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

"""#### Building Decision Tree Model"""

model = DecisionTreeClassifier(random_state=1)
model.fit(X_train, y_train)

"""#### Checking model performance on training set"""

confusion_matrix_sklearn(model, X_train, y_train)

decision_tree_perf_train = model_performance_classification_sklearn(
    model, X_train, y_train
)
decision_tree_perf_train

"""#### Checking model performance on test set"""

confusion_matrix_sklearn(model, X_test, y_test)

decision_tree_perf_test = model_performance_classification_sklearn(model, X_test, y_test)
decision_tree_perf_test

"""The Decision Tree Model performs well on both the Train and Test data sets.There does not appear to be any overfitting. Still, we will prune the model to see if we can get a better model for prediction on the dataset.

**Before pruning the tree let's check the important features.**
"""

feature_names = list(X_train.columns)
importances = model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(8, 8))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color = 'violet', align = 'center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""The graph above shows that the lead time, average room price, market segment, number of special requests, arrival month, and number of week nights are the decision tree model's high impact predictors.

#### Pruning the tree

**Pre-Pruning**
"""

# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1, class_weight="balanced")

# Grid of parameters to choose from
parameters = {
    'max_depth': np.arange(2, 7, 2),
    'max_leaf_nodes': [50, 75, 150, 250],
    'min_samples_split': [10, 30, 50, 70],
}

# Type of scoring used to compare parameter combinations
acc_scorer = make_scorer(f1_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)

"""#### Checking performance on training set"""

confusion_matrix_sklearn(estimator, X_train, y_train)

decision_tree_tune_perf_train = model_performance_classification_sklearn(estimator, X_train, y_train)
decision_tree_tune_perf_train

"""#### Checking performance on test set"""

confusion_matrix_sklearn(estimator, X_test, y_test)

decision_tree_tune_perf_test = model_performance_classification_sklearn(estimator, X_test, y_test)
decision_tree_tune_perf_test

"""#### Visualizing the Decision Tree"""

plt.figure(figsize=(20, 10))
out = tree.plot_tree(
    estimator,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)
# below code will add arrows to the decision tree split if they are missing
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor('black')
        arrow.set_linewidth(1)
plt.show()

# Text report showing the rules of a decision tree -
print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))

# importance of features in the tree building

importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(14, 9))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color = 'violet', align = 'center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""The graph above shows that the lead time, average room price, market segment, number of special requests, arrival month, and number of week nights are the decision tree model's high impact predictors.

**Cost Complexity Pruning**
"""

clf = DecisionTreeClassifier(random_state=1, class_weight = 'balanced')
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = abs(path.ccp_alphas), path.impurities

pd.DataFrame(path)

fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker = "o", drawstyle = 'steps-post')
ax.set_xlabel('Effective alpha')
ax.set_ylabel('Total impurity of leaves')
ax.set_title('Total Impurity vs effective alpha for training set')
plt.show()

"""The impurities in our models and the resulting alphas are shown in the figure above.

The decision tree is then trained using effective alphas. The last value in ccp alphas is the alpha value that eliminates all but one node from the tree
"""

clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(
        random_state=1, ccp_alpha=ccp_alpha, class_weight = 'balanced'
    )
    clf.fit(X_train, y_train)
    clfs.append(clf)
print(
    'Number of nodes in the last tree is: {} with ccp_alpha: {}'.format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1, figsize=(10, 7))
ax[0].plot(ccp_alphas, node_counts, marker= 'o', drawstyle = 'steps-post')
ax[0].set_xlabel('alpha')
ax[0].set_ylabel('number of nodes')
ax[0].set_title('Number of nodes vs alpha')
ax[1].plot(ccp_alphas, depth, marker = 'o', drawstyle = 'steps-post')
ax[1].set_xlabel('alpha')
ax[1].set_ylabel('depth of tree')
ax[1].set_title('Depth vs alpha')
fig.tight_layout()

"""The nodes in our decision tree and the resulting alphas are shown in the graphic above.

The models' maximum depth and the generated alphas are displayed in the second image.

#### F1 Score vs alpha for training and testing sets
"""

f1_train = []
for clf in clfs:
    pred_train = clf.predict(X_train)
    values_train = f1_score(y_train, pred_train)
    f1_train.append(values_train)

f1_test = []
for clf in clfs:
    pred_test = clf.predict(X_test)
    values_test = f1_score(y_test, pred_test)
    f1_test.append(values_test)

fig, ax = plt.subplots(figsize=(15, 5))
ax.set_xlabel('alpha')
ax.set_ylabel('F1 Score')
ax.set_title('F1 Score vs alpha for training and testing sets')
ax.plot(ccp_alphas, f1_train, marker= 'o', label = 'train', drawstyle = 'steps-post')
ax.plot(ccp_alphas, f1_test, marker ='o', label = 'test', drawstyle = 'steps-post')
ax.legend()
plt.show()

"""The F1 score and accompanying alphas for the train and test data are shown in the graphic above. In order to choose the best model for our research, we need to know where on the train and test datasets the model has a reliable measure."""

index_best_model = np.argmax(f1_test)
best_model = clfs[index_best_model]
print(best_model)

"""#### Checking performance on training set"""

confusion_matrix_sklearn(best_model, X_train, y_train)

decision_tree_post_perf_train = model_performance_classification_sklearn(
    best_model, X_train, y_train
)
decision_tree_post_perf_train

"""#### Checking performance on test set"""

confusion_matrix_sklearn(best_model, X_test, y_test)

decision_tree_post_test = model_performance_classification_sklearn(
   best_model, X_test, y_test
)
decision_tree_post_test

plt.figure(figsize=(20, 10))

out = tree.plot_tree(
    best_model,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor('black')
        arrow.set_linewidth(1)
plt.show()

# Text report showing the rules of a decision tree -

print(tree.export_text(best_model, feature_names=feature_names, show_weights = True))

importances = best_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color = 'violet', align = 'center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""#### Comparing Decision Tree models"""

# training performance comparison

models_train_comp_df = pd.concat(
    [
        decision_tree_perf_train.T,
        decision_tree_tune_perf_train.T,
        decision_tree_post_perf_train.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    'Decision Tree sklearn',
    'Decision Tree (Pre-Pruning)',
    'Decision Tree (Post-Pruning)',
]
print('Training performance comparison:')
models_train_comp_df

# testing performance comparison

models_test_comp_df = pd.concat(
    [
        decision_tree_perf_test.T,
        decision_tree_tune_perf_test.T,
        decision_tree_post_test.T,
    ],
    axis=1,
)
models_test_comp_df.columns = [
    'Decision Tree sklearn',
    'Decision Tree (Pre-Pruning)',
    'Decision Tree (Post-Pruning)',
]
print('Testing performance comparison:')
models_test_comp_df

"""# Business Recommendations

### **Conclusion and Recommendations**

The logistic regression model was used to estimate the effects of the predictor variables on the probability of canceling the reservation, the dependent variable is encoded as 1 for canceling and 0 for not canceling.

The results suggest that several factors can influence the likelihood of reservation cancellation. Guests are more likely to cancel their reservations if they are traveling with more people, staying for more nights, require parking space, or have a history of canceling their previous reservations. Moreover, guests are more likely to cancel their reservations if they book a particular meal plan (Meal Plan 2), specific room types or if they belong to the "Corporate" market segment.

On the other hand, guests are less likely to cancel their reservations if they are repeated customers, pay a higher average price per room, book earlier, make fewer special requests, book a different meal plan (Not Selected), or belong to the "Offline" market segment.

For the decision tree model without pre-pruning, the F1 score on the training dataset is higher than on the testing dataset, indicating overfitting. However, the F1 score on both datasets is still relatively high, suggesting that the model is able to balance precision and recall well and achieve good performance.

For the decision tree model with post-pruning, the F1 score on the training dataset is slightly higher than on the testing dataset, indicating good generalization. The F1 score on the testing dataset is also relatively high, indicating good performance.

For the decision tree model with pre-pruning, the F1 score on the training dataset is lower than on the testing dataset, indicating underfitting. The F1 score on the testing dataset is also lower compared to the other models, indicating poorer performance.

Overall, the decision tree model with post-pruning appears to be the best performing model based on the F1 score as it achieves good performance on both the training and testing datasets. The decision tree model without pre-pruning also performs relatively well, but appears to be overfitting to the training data. The decision tree model with pre-pruning appears to be underfitting to the training data and therefore performs worse on the testing dataset.

**Recommendations**

In our logistic regression model as well as the Decision Tree Model, we discovered a number of factors that influence the probability of a reservation being canceled. The results might be used to create policies that make it less likely for bookings to be canceled, such rewarding devoted clients or charging more for reservations with more people or longer stays.

Overall, the model performed well in accurately predicting whether a reservation would be cancelled or not. The high accuracy, recall, precision, and F1 score indicate that the model is effective in identifying canceled reservations and has a low rate of false positives (i.e., predicting a reservation is cancelled when it is not). This high performance is beneficial for hotel management, as it allows them to take appropriate actions to prevent the cancellation of reservations and improve their overall customer

# Appendix

## Univariate Analysis
"""

def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (15,10))
    kde: whether to show the density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows = 2,  # Number of rows of the subplot grid= 2
        sharex = True,  # x-axis will be shared among all subplots
        gridspec_kw = {'height_ratios': (0.25, 0.75)},
        figsize = figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color='yellow'
    )  # boxplot will be created and a triangle will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color='orange', linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color='red', linestyle="-"
    )  # Add median to the histogram

"""**Observations of lead time**"""

histogram_boxplot(data, 'lead_time')

"""**Observations of average room price**"""

histogram_boxplot(data, 'avg_price_per_room')

data.loc[data['avg_price_per_room'] == 0, 'market_segment_type'].value_counts()

# Calculating the 25th quantile
Q1 = data['avg_price_per_room'].quantile(0.25)

# Calculating the 75th quantile
Q3 = data['avg_price_per_room'].quantile(0.75)

# Calculating IQR
IQR = Q3 - Q1

# Calculating value of upper whisker
Upper_Whisker = Q3 + 1.5 * IQR
Upper_Whisker

# assigning the outliers the value of upper whisker
data.loc[data['avg_price_per_room'] >= 500, 'avg_price_per_room'] = Upper_Whisker

"""**Observations of number of previous booking cancellations**"""

histogram_boxplot(data, 'no_of_previous_cancellations')

"""**Observations of number of previous booking not canceled**

histogram & boxplot for number of previous booking not canceled
"""

histogram_boxplot(data,'no_of_previous_bookings_not_canceled')

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 2, 6))
    else:
        plt.figure(figsize=(n + 2, 6))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data = data,
        x = feature,
        palette = 'tab10',
        order = data[feature].value_counts().index[:n],
    )

    for p in ax.patches:
        if perc == True:
            label = '{:.1f}%'.format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha= 'center',
            va= 'center',
            size = 12,
            xytext =  (0, 5),
            textcoords = 'offset points',
        )  # annotate the percentage

    plt.show()  # show the plot

"""**Observations of number of adults**



"""

labeled_barplot(data, "no_of_adults", perc=True)

"""**Observations of number of children**

barplot for number of children
"""

labeled_barplot(data, 'no_of_children')

# replacing 9, and 10 children with 3
data['no_of_children'] = data['no_of_children'].replace([9, 10], 3)

"""**Observations of number of week nights**





"""

labeled_barplot(data,'no_of_week_nights')

"""**Observations of number of weekend nights**"""

labeled_barplot(data, 'no_of_weekend_nights')

"""**Observations of required car parking space**"""

labeled_barplot(data, 'required_car_parking_space')

"""
**Observations of type of meal plan**"""

labeled_barplot(data,'type_of_meal_plan')

"""**Observations of room type reserved**


"""

labeled_barplot(data, 'room_type_reserved')

"""**Observations of arrival month**"""

labeled_barplot(data, 'arrival_month')

"""### Observations on market segment type"""

labeled_barplot(data, 'market_segment_type')

"""
**Observations ofnumber of special requests**"""

labeled_barplot(data, 'no_of_special_requests')

"""## Bivariate Analysis

**Creating functions that will help us with further analysis.**
"""

### function to plot distributions wrt target


def distribution_plot_wrt_target(data, predictor, target):

    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    target_uniq = data[target].unique()

    axs[0, 0].set_title('Distribution of target for target=' + str(target_uniq[0]))
    sns.histplot(
        data = data[data[target] == target_uniq[0]],
        x = predictor,
        kde = True,
        ax= axs[0, 0],
        color = 'teal',
        stat  = 'density',
    )

    axs[0, 1].set_title('Distribution of target for target=' + str(target_uniq[1]))
    sns.histplot(
        data=data[data[target] == target_uniq[1]],
        x = predictor,
        kde = True,
        ax =axs[0, 1],
        color = 'orange',
        stat = 'density',
    )

    axs[1, 0].set_title('Boxplot w.r.t target')
    sns.boxplot(data = data, x = target, y = predictor, ax = axs[1, 0], palette = 'gist_rainbow')

    axs[1, 1].set_title('Boxplot (without outliers) w.r.t target')
    sns.boxplot(
        data = data,
        x = target,
        y = predictor,
        ax = axs[1, 1],
        showfliers = False,
        palette = 'gist_rainbow',
    )

    plt.tight_layout()
    plt.show()

def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print('-' * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize = 'index').sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind='bar', stacked = False, figsize=(count + 5, 5))
    plt.legend(
        loc = 'lower left', frameon = False,
    )
    plt.legend(loc = 'upper left', bbox_to_anchor=(1, 1))
    plt.show()

"""***Relations of Average room price & market segments***"""

plt.figure(figsize=(10, 6))
sns.boxplot(
    data = data, x = 'market_segment_type', y = 'avg_price_per_room', palette = 'gist_rainbow'
)
plt.show()

"""**Cancellation across market segments**"""

stacked_barplot(data, 'market_segment_type', 'booking_status')

"""**Impacts cancellations on special requirements**"""

stacked_barplot(data, 'no_of_special_requests', 'booking_status')

"""**Impacts the prices of a room on special requests**"""

plt.figure(figsize=(10, 5))
sns.boxplot(data = data, x = 'no_of_special_requests', y = 'avg_price_per_room')
plt.show()

"""**Correlation between booking status and average price per room.**"""

distribution_plot_wrt_target(data, 'avg_price_per_room', 'booking_status')

"""**Correlation between booking status and lead time also**"""

distribution_plot_wrt_target(data, 'lead_time', 'booking_status')

"""**The impact on cancellation of the customers who traveled with their families.**"""

family_data = data[(data['no_of_children'] >= 0) & (data['no_of_adults'] > 1)]
family_data.shape

family_data['no_of_family_members'] = (
    family_data['no_of_adults'] + family_data['no_of_children']
)

stacked_barplot(family_data, 'no_of_family_members', 'booking_status')

"""**Customer who stay for at least a day/night at the hotel**"""

stay_data = data[(data['no_of_week_nights'] > 0)|(data['no_of_weekend_nights'] > 0)]
stay_data.shape

stay_data['total_days'] = (
    stay_data['no_of_week_nights'] + stay_data['no_of_weekend_nights']
)

stacked_barplot(stay_data, 'total_days', 'booking_status')

"""**Percentage of repeating guests cancellation?**"""

stacked_barplot(data, 'repeated_guest', 'booking_status')

"""**The busiest months in the hotel**





"""

# grouping the data on arrival months and extracting the count of bookings
monthly_data = data.groupby(['arrival_month'])['booking_status'].count()

# creating a dataframe with months and count of customers in each month
monthly_data = pd.DataFrame(
    {'Month': list(monthly_data.index), 'Guests': list(monthly_data.values)}
)

# plotting the trend over different months
plt.figure(figsize=(10, 5))
sns.lineplot(data=monthly_data, x= 'Month', y = 'Guests')
plt.show()

"""**Percentage of bookings canceled in each month.**





"""

stacked_barplot(data, 'arrival_month', 'booking_status')

"""**Varying prices across different months**"""

plt.figure(figsize=(10, 5))
sns.lineplot(data = data, y = 'avg_price_per_room', x = 'arrival_month', )
plt.show()